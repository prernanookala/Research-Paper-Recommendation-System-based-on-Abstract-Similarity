ğŸ“š Research Paper Recommendation System based on Abstract Similarity

ğŸ† Project Overview
Finding relevant academic papers is challenging with the ever-growing volume of research. This system solves that problem by providing accurate and context-aware recommendations based on abstract similarity.

âœ… Natural Language Processing (NLP) based recommendations
âœ… State-of-the-art Embeddings (SBERT, AllenAI-Specter, TF-IDF)
âœ… Fast retrieval with LanceDB for large datasets
âœ… Explainability with RAG (Retrieval-Augmented Generation)

ğŸš€ Key Features
ğŸ”¹ Abstract Similarity Matching - Finds the most relevant papers based on semantic similarity.
ğŸ”¹ Hybrid Scoring Mechanism - Considers category, similarity, clustering, and temporal relevance.
ğŸ”¹ Fast & Scalable Retrieval - Uses LanceDB to store and query high-dimensional embeddings efficiently.
ğŸ”¹ Explainable AI - Google FLAN-T5 generates human-readable explanations for recommendations.
ğŸ”¹ Support for Large-Scale Datasets - Handles millions of research papers effectively.

ğŸ—ï¸ System Architecture

ğŸ›  Technologies Used
Python 3.8+
Hugging Face Transformers (SBERT, AllenAI-Specter, FLAN-T5)
LanceDB (Vector storage & retrieval)
Scikit-learn (TF-IDF, Clustering)
Streamlit (User Interface)

ğŸ” How it Works
User inputs an abstract.
Preprocessing: Cleaning & feature extraction.
Embedding Generation: Using TF-IDF, SBERT, or AllenAI-Specter.
Similarity Matching: Finds the closest abstracts using cosine similarity.
Hybrid Scoring: Adjusts ranking based on category, clustering, and temporal relevance.
RAG Explainability: Generates human-like explanations for recommendations.
Results Displayed! ğŸ‰

ğŸ“Š Dataset Details
ğŸ“– Source: ArXiv Metadata Snapshot

ğŸ“ Size: 2.6M+ research papers across 149 categories

ğŸ“Š Filtered Sample: ~100,000 papers for high relevance

ğŸ” Fields Used:
Title, Authors, Categories, Abstract, Publication Date

ğŸ›  Preprocessing Steps:

Stratified Sampling to ensure category balance.

Text Cleaning (stopword removal, lowercasing, lemmatization).

TF-IDF and Embedding Generation for accurate recommendations.

ğŸ“ˆ Performance & Evaluation

âœ… Evaluation Methods:
âœ” Category Matching - Measures how well recommendations match the input category.
âœ” Clustering-Based Similarity - Uses K-Means clustering for grouping similar papers.
âœ” Temporal Relevance - Prioritizes recent publications.
âœ” Hybrid Scoring - Combines multiple factors for best recommendations.

ğŸ“Š Metrics Used:
Precision@k

Recall@k

Mean Reciprocal Rank (MRR)

ğŸ† Key Findings:
ğŸ“Œ AllenAI-Specter embeddings delivered the most accurate recommendations.
ğŸ“Œ Fine-tuned SBERT performed well but was task-specific.
ğŸ“Œ TF-IDF Baseline provided quick results but lacked deep semantic understanding.
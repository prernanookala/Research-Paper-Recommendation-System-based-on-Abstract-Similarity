{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing necessary packages"
      ],
      "metadata": {
        "id": "VyIjOzvAucRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lancedb\n",
        "!pip install sentence_transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "911PvtVuuaBk",
        "outputId": "712186c2-5552-4ee4-9fcf-4e2e077bd7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lancedb in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.10/dist-packages (from lancedb) (2.1.0)\n",
            "Requirement already satisfied: pylance==0.20.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (0.20.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (4.66.6)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from lancedb) (2.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lancedb) (24.2)\n",
            "Requirement already satisfied: overrides>=0.7 in /usr/local/lib/python3.10/dist-packages (from lancedb) (7.7.0)\n",
            "Requirement already satisfied: pyarrow>=14 in /usr/local/lib/python3.10/dist-packages (from pylance==0.20.0->lancedb) (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from pylance==0.20.0->lancedb) (1.26.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->lancedb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->lancedb) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->lancedb) (4.12.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Drive to access the JSON dataset file\n"
      ],
      "metadata": {
        "id": "xCNOvBBJsVyy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGwofNpewtyf",
        "outputId": "8811d33f-a4b4-437b-dad6-f50f37a63122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.bag as db\n",
        "import json\n",
        "data = db.read_text('/content/drive/MyDrive/arxiv-metadata-oai-snapshot.json').map(json.loads)"
      ],
      "metadata": {
        "id": "hZgjYZyFsycC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4_W8GbWs6VJ",
        "outputId": "1e79fc56-690d-46b7-aa9b-7e304444be4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'id': '0704.0001',\n",
              "  'submitter': 'Pavel Nadolsky',\n",
              "  'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
              "  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
              "  'comments': '37 pages, 15 figures; published version',\n",
              "  'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
              "  'doi': '10.1103/PhysRevD.76.013009',\n",
              "  'report-no': 'ANL-HEP-PR-07-12',\n",
              "  'categories': 'hep-ph',\n",
              "  'license': None,\n",
              "  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
              "   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
              "  'update_date': '2008-11-26',\n",
              "  'authors_parsed': [['BalÃ¡zs', 'C.', ''],\n",
              "   ['Berger', 'E. L.', ''],\n",
              "   ['Nadolsky', 'P. M.', ''],\n",
              "   ['Yuan', 'C. -P.', '']]},)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Stratified Sample of 100K records based on the category in the dataset"
      ],
      "metadata": {
        "id": "jN5MW687seDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the file path\n",
        "file_name = '/content/drive/MyDrive/arxiv-metadata-oai-snapshot.json'\n",
        "\n",
        "# Define the columns to extract\n",
        "cols = ['id', 'title', 'abstract', 'categories', 'authors', 'comments', 'update_date']\n",
        "\n",
        "# Initialize a Counter to store unique categories\n",
        "category_counter = Counter()\n",
        "\n",
        "# Load data and extract relevant fields + count categories\n",
        "data = []\n",
        "with open(file_name, encoding='latin-1') as f:\n",
        "    for line in f:\n",
        "        doc = json.loads(line)\n",
        "        categories = doc.get('categories', '').strip()\n",
        "        if categories and ' ' not in categories:  # Include only rows with a single category\n",
        "            category_counter.update([categories])\n",
        "            data.append([\n",
        "                doc.get('id'),\n",
        "                doc.get('title', ''),\n",
        "                doc.get('abstract', ''),\n",
        "                categories,\n",
        "                doc.get('authors', ''),\n",
        "                doc.get('comments', ''),\n",
        "                doc.get('update_date', '')\n",
        "            ])\n",
        "\n",
        "# Print unique categories\n",
        "unique_categories = list(category_counter.keys())\n",
        "print(f\"Total unique categories: {len(unique_categories)}\")\n",
        "print(\"Unique categories:\")\n",
        "print(unique_categories)\n",
        "\n",
        "# Optionally, print the top 20 most common categories with their counts\n",
        "print(\"\\nTop 20 categories by frequency:\")\n",
        "for category, count in category_counter.most_common(20):\n",
        "    print(f\"{category}: {count}\")\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data, columns=cols)\n",
        "\n",
        "# Clean the DataFrame\n",
        "# df['abstract'] = df['abstract'].str.strip().str.lower()\n",
        "# df['title'] = df['title'].str.strip().str.lower()\n",
        "# df['categories'] = df['categories'].str.strip()\n",
        "# df['authors'] = df['authors'].str.strip()\n",
        "# df['comments'] = df['comments'].str.strip()\n",
        "df['update_date'] = pd.to_datetime(df['update_date'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing abstracts or titles\n",
        "df = df.dropna(subset=['abstract', 'title'])\n",
        "\n",
        "# Filter out categories with fewer than 10 samples\n",
        "category_counts = df['categories'].value_counts()\n",
        "valid_categories = category_counts[category_counts >= 50].index\n",
        "print(f\"\\nTotal valid categories: {len(valid_categories)}\")\n",
        "\n",
        "# Filter the DataFrame to include only valid categories\n",
        "df_filtered = df[df['categories'].isin(valid_categories)]\n",
        "\n",
        "# Stratified sampling based on categories\n",
        "if len(df_filtered) < 100000:\n",
        "    raise ValueError(f\"Not enough data to sample 100,000 rows. Available: {len(df_filtered)}\")\n",
        "\n",
        "stratified_sample, _ = train_test_split(\n",
        "    df_filtered,\n",
        "    train_size=100000,\n",
        "    stratify=df_filtered['categories'],\n",
        "    random_state=62\n",
        ")\n",
        "\n",
        "# Reset index\n",
        "stratified_sample = stratified_sample.reset_index(drop=True)\n",
        "\n",
        "# Save the final sample to a CSV file\n",
        "stratified_sample.to_csv('stratified_sample.csv', index=False)\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\nFinal dataset size: {len(stratified_sample)}\")\n",
        "print(\"Category distribution in the sample:\")\n",
        "print(stratified_sample['categories'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhgnHbhPw6xa",
        "outputId": "b23b3898-6d49-413d-f3aa-9635719bda74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique categories: 149\n",
            "Unique categories:\n",
            "['hep-ph', 'physics.gen-ph', 'math.CO', 'cond-mat.mes-hall', 'gr-qc', 'cond-mat.mtrl-sci', 'astro-ph', 'math.NT', 'hep-th', 'hep-ex', 'math.NA', 'nlin.PS', 'math.RA', 'cond-mat.str-el', 'physics.pop-ph', 'nucl-th', 'math.FA', 'cs.DS', 'math.DS', 'physics.soc-ph', 'math.AG', 'math.OA', 'math.PR', 'math.DG', 'physics.optics', 'math.GR', 'nlin.SI', 'math.SG', 'physics.data-an', 'cs.CC', 'math.GT', 'quant-ph', 'cond-mat.other', 'math.CV', 'math.AP', 'cond-mat.supr-con', 'math.RT', 'cond-mat.stat-mech', 'q-bio.OT', 'physics.plasm-ph', 'nlin.CG', 'nucl-ex', 'cond-mat.soft', 'physics.comp-ph', 'math.MG', 'math.QA', 'physics.bio-ph', 'physics.chem-ph', 'math.AT', 'physics.geo-ph', 'q-bio.BM', 'math.OC', 'cs.CR', 'physics.class-ph', 'q-bio.PE', 'q-bio.NC', 'physics.atom-ph', 'math.GM', 'hep-lat', 'math.CA', 'physics.atm-clus', 'cs.PF', 'physics.acc-ph', 'math.SP', 'nlin.CD', 'physics.hist-ph', 'physics.flu-dyn', 'cond-mat.dis-nn', 'cs.CV', 'cs.LG', 'cs.SE', 'physics.ed-ph', 'physics.ins-det', 'cs.PL', 'q-bio.CB', 'cs.AI', 'math.LO', 'cs.LO', 'stat.AP', 'nlin.AO', 'cs.DC', 'cs.DM', 'cs.NI', 'cs.OH', 'q-bio.GN', 'q-bio.MN', 'math.KT', 'math.AC', 'physics.med-ph', 'cs.HC', 'cs.NE', 'physics.ao-ph', 'math.HO', 'q-bio.QM', 'cs.MS', 'physics.space-ph', 'cs.DB', 'cs.CL', 'math.CT', 'cs.CE', 'cs.GT', 'cs.CY', 'cs.CG', 'stat.ME', 'cs.RO', 'cs.GL', 'cs.MA', 'math.GN', 'cs.IR', 'stat.ML', 'cs.DL', 'cs.SC', 'q-bio.SC', 'stat.CO', 'cs.NA', 'cs.AR', 'cs.OS', 'q-bio.TO', 'cs.GR', 'cs.MM', 'cs.SD', 'cond-mat.quant-gas', 'q-fin.GN', 'q-fin.ST', 'q-fin.PM', 'q-fin.PR', 'q-fin.CP', 'q-fin.RM', 'astro-ph.HE', 'astro-ph.SR', 'astro-ph.GA', 'astro-ph.CO', 'astro-ph.IM', 'astro-ph.EP', 'q-fin.TR', 'cs.FL', 'stat.OT', 'cs.SY', 'cs.SI', 'cs.ET', 'eess.SP', 'q-fin.EC', 'q-fin.MF', 'physics.app-ph', 'econ.TH', 'eess.IV', 'econ.EM', 'eess.AS', 'cond-mat']\n",
            "\n",
            "Top 20 categories by frequency:\n",
            "astro-ph: 86911\n",
            "hep-ph: 82167\n",
            "quant-ph: 71432\n",
            "cs.CV: 64093\n",
            "hep-th: 59556\n",
            "cond-mat.mtrl-sci: 40433\n",
            "cond-mat.mes-hall: 35619\n",
            "math.AP: 35554\n",
            "astro-ph.GA: 31968\n",
            "gr-qc: 31174\n",
            "math.CO: 30823\n",
            "astro-ph.SR: 27442\n",
            "cs.CL: 27406\n",
            "cond-mat.str-el: 26581\n",
            "math.PR: 24283\n",
            "astro-ph.HE: 23836\n",
            "math.NT: 22721\n",
            "astro-ph.CO: 22433\n",
            "math.AG: 20780\n",
            "cond-mat.stat-mech: 19832\n",
            "\n",
            "Total valid categories: 149\n",
            "\n",
            "Final dataset size: 100000\n",
            "Category distribution in the sample:\n",
            "categories\n",
            "astro-ph    6277\n",
            "hep-ph      5934\n",
            "quant-ph    5159\n",
            "cs.CV       4629\n",
            "hep-th      4301\n",
            "            ... \n",
            "cs.OS         23\n",
            "nlin.CG       17\n",
            "q-bio.SC      16\n",
            "q-fin.EC      15\n",
            "cs.GL          5\n",
            "Name: count, Length: 149, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering the sample further to exclude categories with unique counts less than 20 to maintain a stratified distribution throughout"
      ],
      "metadata": {
        "id": "MA889nqos_Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your DataFrame\n",
        "df = pd.read_csv('stratified_sample.csv')\n",
        "\n",
        "# Get category counts\n",
        "category_counts = df['categories'].value_counts()\n",
        "\n",
        "# Identify categories with 20 or more occurrences\n",
        "valid_categories = category_counts[category_counts >= 20].index\n",
        "\n",
        "# Filter DataFrame to keep only rows with valid categories\n",
        "df_filtered = df[df['categories'].isin(valid_categories)]\n",
        "\n",
        "# Display the number of rows before and after filtering\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "print(f\"Filtered dataset size: {len(df_filtered)}\")\n",
        "\n",
        "# Save the filtered DataFrame to a new CSV file\n",
        "df_filtered.to_csv('filtered_stratified_sample.csv', index=False)\n",
        "\n",
        "# Display summary\n",
        "print(\"Categories with fewer than 20 records have been removed.\")\n",
        "print(\"Updated category distribution:\")\n",
        "print(df_filtered['categories'].value_counts())\n"
      ],
      "metadata": {
        "id": "tB1JPAGVxIYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcda399f-4e3f-487e-8c70-438b89e1cc5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 100000\n",
            "Filtered dataset size: 99947\n",
            "Categories with fewer than 20 records have been removed.\n",
            "Updated category distribution:\n",
            "categories\n",
            "astro-ph    6277\n",
            "hep-ph      5934\n",
            "quant-ph    5159\n",
            "cs.CV       4629\n",
            "hep-th      4301\n",
            "            ... \n",
            "cs.SD         27\n",
            "cs.NA         26\n",
            "cs.MS         26\n",
            "q-bio.CB      25\n",
            "cs.OS         23\n",
            "Name: count, Length: 145, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the sample into Train(70K), Validation(15K), Test(15K)"
      ],
      "metadata": {
        "id": "grByYJCNtmNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the stratified sample\n",
        "df = df_filtered\n",
        "\n",
        "# Split into train (70k) and remaining (30k)\n",
        "train_df, remaining_df = train_test_split(\n",
        "    df,\n",
        "    train_size=70000,\n",
        "    stratify=df['categories'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split remaining into validation (15k) and test (15k)\n",
        "val_df, test_df = train_test_split(\n",
        "    remaining_df,\n",
        "    test_size=0.5,\n",
        "    stratify=remaining_df['categories'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Save the splits to CSV files\n",
        "train_df.to_csv('train_df.csv', index=False)\n",
        "val_df.to_csv('val_df.csv', index=False)\n",
        "test_df.to_csv('test_df.csv', index=False)\n",
        "\n",
        "# Display summary\n",
        "print(f\"Train set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n"
      ],
      "metadata": {
        "id": "L61fzH5pxL-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96a7813-b375-465e-d38a-a9bef0489d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 70000\n",
            "Validation set size: 14973\n",
            "Test set size: 14974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data by performing necessary cleaning operations (Lowercasing, Lemmatizing, Removing punctuations, whitespace, special characters)"
      ],
      "metadata": {
        "id": "z-aUKsfCtxDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load the stratified sample dataset\n",
        "df = pd.read_csv('train_df.csv')\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    # Remove extra whitespace and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Tokenize and remove stopwords, then lemmatize\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply cleaning to relevant fields\n",
        "df['cleaned_title'] = df['title'].apply(clean_text)\n",
        "df['cleaned_authors'] = df['authors'].apply(clean_text)\n",
        "df['cleaned_categories'] = df['categories'].apply(clean_text)\n",
        "df['cleaned_abstract'] = df['abstract'].apply(clean_text)\n",
        "df['cleaned_comments'] = df['comments'].apply(clean_text)\n",
        "\n",
        "# Create the enhanced text field and remove newlines\n",
        "df['enhanced_text'] = df.apply(lambda row: f\"\"\"\n",
        "Title: {row['cleaned_title']} [SEP]\n",
        "Authors: {row['cleaned_authors']} [SEP]\n",
        "Categories: {row['cleaned_categories']} [SEP]\n",
        "Abstract: {row['cleaned_abstract']} [SEP]\n",
        "Comments: {row['cleaned_comments']} [SEP]\n",
        "Updated on: {row['update_date']}\n",
        "\"\"\".replace('\\n', ' ').strip(), axis=1)\n",
        "\n",
        "# Display the first few rows to verify the enhanced text field\n",
        "print(df[['id', 'enhanced_text']].head())\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "df.to_csv('enhanced_stratified_sample_train.csv', index=False)\n",
        "\n",
        "print(\"Enhanced text field created and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNuUsAwtylwb",
        "outputId": "95ba99c7-2daa-4eab-a28d-21996db7194b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 id                                      enhanced_text\n",
            "0        2007.12657  Title: sublimative evolution 486958 arrokoth [...\n",
            "1         1208.4774  Title: torii phase [SEP] Authors: emmanuel ami...\n",
            "2         0903.4882  Title: kinetic monte carlo simulation strained...\n",
            "3        1601.06809  Title: test field cannot destroy extremal blac...\n",
            "4  astro-ph/0104478  Title: low albedo among extinct comet candidat...\n",
            "Enhanced text field created and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the schema for storing the sentence transformer embeddings in LanceDB"
      ],
      "metadata": {
        "id": "QeVXmfhMuizh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lancedb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from lancedb.embeddings import get_registry\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "\n",
        "# Load the enhanced dataset\n",
        "df = pd.read_csv(\"enhanced_stratified_sample_train.csv\")\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df.head())\n",
        "\n",
        "# Load the Sentence-Transformer model\n",
        "model_name = \"all-MiniLM-L6-v2\"  # You can adjust the model based on your use case\n",
        "db = lancedb.connect(\"lancedb_directory\")\n",
        "\n",
        "# Register the embedding function\n",
        "registry = get_registry()\n",
        "embedding_function = registry.get(\"sentence-transformers\").create(\n",
        "    name=model_name,\n",
        "    device=\"cuda\"  # Use \"cuda\" for GPU; use \"cpu\" if GPU is not available\n",
        ")\n",
        "\n",
        "# Define the LanceDB schema with Pydantic\n",
        "class TextData(LanceModel):\n",
        "    id: str\n",
        "    title: str\n",
        "    authors: str\n",
        "    abstract: str\n",
        "    categories: str\n",
        "    comments: str\n",
        "    update_date: str\n",
        "    enhanced_text: str = embedding_function.SourceField()  # Source text for embeddings\n",
        "    embedding: Vector(embedding_function.ndims()) = embedding_function.VectorField()\n",
        "\n",
        "# Create the table (overwrite if it exists)\n",
        "table = db.create_table(\"enhanced_papers\", schema=TextData, mode=\"overwrite\")\n",
        "\n",
        "# Convert the DataFrame to a list of dictionaries\n",
        "data = df[[\"id\", \"title\", \"authors\", \"categories\", \"comments\", \"update_date\", \"enhanced_text\"]].astype(str).to_dict(orient=\"records\")\n",
        "\n",
        "# Add data to the table\n",
        "table.add(data)\n",
        "\n",
        "print(\"Enhanced data added to the LanceDB table successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-91Oyk9ly9Qr",
        "outputId": "94b0f759-492e-4948-f01c-454518d38aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 id                                              title  \\\n",
            "0        2007.12657     The Sublimative Evolution of (486958) Arrokoth   \n",
            "1         1208.4774                                The Torii of phases   \n",
            "2         0903.4882  Kinetic Monte Carlo Simulation of Strained Het...   \n",
            "3        1601.06809    Test fields cannot destroy extremal black holes   \n",
            "4  astro-ph/0104478         Low Albedos Among Extinct Comet Candidates   \n",
            "\n",
            "                                            abstract         categories  \\\n",
            "0    We consider the history of New Horizons targ...        astro-ph.EP   \n",
            "1    The import of the magnitude of fourier coeff...            math.HO   \n",
            "2    An efficient method for the simulation of st...  cond-mat.mtrl-sci   \n",
            "3    We prove that (possibly charged) test fields...              gr-qc   \n",
            "4    We present radiometric effective radii and v...           astro-ph   \n",
            "\n",
            "                                             authors  \\\n",
            "0  Jordan K. Steckloff, Carey M. Lisse, Taylor K....   \n",
            "1                                     Emmanuel Amiot   \n",
            "2       Arvind Baskaran, Jason Devita, Peter Smereka   \n",
            "3  Jose Natario, Leonel Queimada and Rodrigo Vicente   \n",
            "4      Y. R. Fernandez, D. C. Jewitt, S. S. Sheppard   \n",
            "\n",
            "                                            comments update_date  \\\n",
            "0                       19 pages, 2 figures, 1 table  2021-01-20   \n",
            "1                                                NaN  2015-03-20   \n",
            "2                                                NaN  2015-05-13   \n",
            "3  14 pages, 1 figure; v2: major rewrite, referen...  2016-08-30   \n",
            "4  12 pages + 1 figure, AASTeX v5.0 preprint form...  2009-11-06   \n",
            "\n",
            "                                       cleaned_title  \\\n",
            "0              sublimative evolution 486958 arrokoth   \n",
            "1                                        torii phase   \n",
            "2  kinetic monte carlo simulation strained hetero...   \n",
            "3      test field cannot destroy extremal black hole   \n",
            "4           low albedo among extinct comet candidate   \n",
            "\n",
            "                                     cleaned_authors cleaned_categories  \\\n",
            "0  jordan k steckloff carey lisse taylor k safrit...          astrophep   \n",
            "1                                     emmanuel amiot             mathho   \n",
            "2         arvind baskaran jason devita peter smereka     condmatmtrlsci   \n",
            "3       jose natario leonel queimada rodrigo vicente               grqc   \n",
            "4                      r fernandez c jewitt sheppard            astroph   \n",
            "\n",
            "                                    cleaned_abstract  \\\n",
            "0  consider history new horizon target 486958 arr...   \n",
            "1  import magnitude fourier coefficient pitch cla...   \n",
            "2  efficient method simulation strained heteroepi...   \n",
            "3  prove possibly charged test field satisfying n...   \n",
            "4  present radiometric effective radius visual ge...   \n",
            "\n",
            "                                    cleaned_comments  \\\n",
            "0                           19 page 2 figure 1 table   \n",
            "1                                                NaN   \n",
            "2                                                NaN   \n",
            "3  14 page 1 figure v2 major rewrite reference ad...   \n",
            "4  12 page 1 figure aastex v50 preprint format ac...   \n",
            "\n",
            "                                       enhanced_text  \n",
            "0  Title: sublimative evolution 486958 arrokoth [...  \n",
            "1  Title: torii phase [SEP] Authors: emmanuel ami...  \n",
            "2  Title: kinetic monte carlo simulation strained...  \n",
            "3  Title: test field cannot destroy extremal blac...  \n",
            "4  Title: low albedo among extinct comet candidat...  \n",
            "Enhanced data added to the LanceDB table successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sklearn.cluster import KMeans\n",
        "# from datetime import datetime\n",
        "# import lancedb\n",
        "\n",
        "# # Load the SBERT model\n",
        "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # Connect to LanceDB and open the table\n",
        "# db = lancedb.connect(\"lancedb_directory\")\n",
        "# table = db.open_table(\"enhanced_papers\")\n",
        "\n",
        "# # Load the training data from CSV (for metadata like categories)\n",
        "# train_df = pd.read_csv(\"train_df.csv\")\n",
        "\n",
        "# # Fetch embeddings from LanceDB for the training data\n",
        "# embedding_index = table.to_pandas().columns.get_loc('embedding')\n",
        "# train_embeddings = np.vstack([row[embedding_index + 1] for row in table.to_pandas().itertuples() if row.id in train_df['id'].values])\n",
        "\n",
        "# # Perform clustering on train embeddings for clustering-based ground truth\n",
        "# num_clusters = 20  # Adjust the number of clusters as needed\n",
        "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "# train_df['cluster'] = kmeans.fit_predict(train_embeddings)"
      ],
      "metadata": {
        "id": "G-p4-w1hPevH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning & preprocessing the test data"
      ],
      "metadata": {
        "id": "DT1lviP7u8j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load the stratified sample dataset\n",
        "df = pd.read_csv('test_df.csv')\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    # Remove extra whitespace and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Tokenize and remove stopwords, then lemmatize\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "df['cleaned_abstract'] = df['abstract'].apply(clean_text)\n",
        "\n",
        "\n",
        "test_df = df\n"
      ],
      "metadata": {
        "id": "eibLO6CkPkn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d844404c-0456-45ca-fc64-ec8e4af6c450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import lancedb\n",
        "\n",
        "# Load the SBERT model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Connect to LanceDB and open the table\n",
        "db = lancedb.connect(\"lancedb_directory\")\n",
        "table = db.open_table(\"enhanced_papers\")\n",
        "\n",
        "# Input abstract for which recommendations are needed\n",
        "user_abstract = \"\"\"zeroshot quantization zsq promising compressing accelerating deep neural networks data training\n",
        "fullprecision models inaccessible zsq network quantization performed using synthetic samples performance quantized\n",
        "models depends heavily quality synthetic samples nonetheless synthetic samples constructed existing zsq methods\n",
        "easily fitted models accordingly quantized models obtained methods suffer significant performance degradation\n",
        "hard samples address issue propose hard sample synthesizing training hast specifically hast pays attention hard\n",
        "samples synthesizing samples makes synthetic samples hard fit training quantized models hast aligns features\n",
        "extracted fullprecision quantized models ensure similarity features extracted models extensive experiments hast\n",
        "significantly outperforms existing zsq methods achieving performance comparable models quantized real data\"\"\"\n",
        "\n",
        "# Generate embedding for the input abstract\n",
        "user_embedding = model.encode(user_abstract, convert_to_tensor=True).cpu().numpy()\n",
        "\n",
        "# Perform similarity search in LanceDB with a limit of 5 recommendations\n",
        "recommendations = table.search(user_embedding).metric(\"cosine\").limit(5).to_pandas()\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "recommendation_vectors = np.vstack(recommendations['embedding'].tolist())\n",
        "cosine_similarities = cosine_similarity(user_embedding.reshape(1, -1), recommendation_vectors)[0]\n",
        "\n",
        "# Add similarity scores to the recommendations DataFrame\n",
        "recommendations['similarity_score'] = cosine_similarities\n",
        "\n",
        "# Display recommendations with title, abstract, and similarity score\n",
        "print(\"\\nTop Recommendations:\")\n",
        "for idx, row in recommendations.iterrows():\n",
        "    print(f\"\\nRecommendation {idx + 1}:\")\n",
        "    print(f\"Title: {row['title']}\")\n",
        "    print(f\"Abstract: {row['abstract']}\")\n",
        "    print(f\"Similarity Score: {row['similarity_score']:.4f}\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "LbSGWQSc68s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Testing with all the ground truth measures"
      ],
      "metadata": {
        "id": "pL4Hey5NvQw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from datetime import datetime\n",
        "import lancedb\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "\n",
        "# Load the SBERT model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Connect to LanceDB and open the table\n",
        "db = lancedb.connect(\"lancedb_directory\")\n",
        "table = db.open_table(\"enhanced_papers\")\n",
        "\n",
        "# Load the training data from CSV (for metadata like categories)\n",
        "train_df = pd.read_csv(\"train_df.csv\")\n",
        "\n",
        "# Perform clustering on train embeddings for clustering-based ground truth\n",
        "train_embeddings = np.vstack(table.to_pandas()[\"embedding\"].tolist())\n",
        "num_clusters = 20  # Adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "train_df['cluster'] = kmeans.fit_predict(train_embeddings)\n",
        "\n",
        "# Define temporal evaluation parameters\n",
        "time_window_days = 365  # 1-year window for temporal evaluation\n",
        "\n",
        "# Similarity threshold\n",
        "similarity_threshold = 0.7\n",
        "\n",
        "# Weights for hybrid scoring\n",
        "weights = {\"category\": 0.3, \"cluster\": 0.2, \"similarity\": 0.3, \"temporal\": 0.2}\n",
        "\n",
        "# Precompute category counts in the training data\n",
        "category_counts = train_df[\"categories\"].value_counts().to_dict()\n",
        "\n",
        "# Initialize metrics for each ground truth method and k values\n",
        "k_values = [5, 10, 15]\n",
        "metrics = {\n",
        "    k: {\n",
        "        \"category\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "        \"clustering\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "        \"similarity\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "        \"temporal\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "        \"hybrid\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "    }\n",
        "    for k in k_values\n",
        "}\n",
        "\n",
        "# Number of queries to process\n",
        "num_queries = len(test_df)\n",
        "\n",
        "# Lock for thread-safe metric updates\n",
        "lock = threading.Lock()\n",
        "\n",
        "# Evaluation functions\n",
        "def precision_at_k(recommendations, true_label, k):\n",
        "    relevant = sum(1 for label in recommendations[\"categories\"].tolist()[:k] if label == true_label)\n",
        "    return relevant / k\n",
        "\n",
        "def recall_at_k(recommendations, true_label, all_relevant_count, k):\n",
        "    relevant = sum(1 for label in recommendations[\"categories\"].tolist()[:k] if label == true_label)\n",
        "    return relevant / all_relevant_count if all_relevant_count > 0 else 0\n",
        "\n",
        "def mean_reciprocal_rank(recommendations, true_label, k):\n",
        "    for i, label in enumerate(recommendations[\"categories\"].tolist()[:k]):\n",
        "        if label == true_label:\n",
        "            return 1 / (i + 1)\n",
        "    return 0\n",
        "\n",
        "def temporal_score(query_date, rec_date, window=time_window_days):\n",
        "    rec_date = pd.to_datetime(rec_date, errors='coerce')\n",
        "    return 1 if pd.notnull(rec_date) and abs((query_date - rec_date).days) <= window else 0\n",
        "\n",
        "# Function to process a single query\n",
        "def process_query(idx, query, true_category, query_date):\n",
        "    local_metrics = {k: {method: {\"precision\": 0, \"recall\": 0, \"mrr\": 0} for method in metrics[k]} for k in k_values}\n",
        "\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = model.encode(query, batch_size=32, show_progress_bar=False)\n",
        "\n",
        "    # Perform similarity search in LanceDB with a limit of 15\n",
        "    recommendations = table.search(query_embedding).metric(\"cosine\").limit(15).to_pandas()\n",
        "    recommendation_vectors = np.vstack(recommendations['embedding'].tolist())\n",
        "\n",
        "    # Precompute true cluster and all relevant count\n",
        "    all_relevant_count = category_counts.get(true_category, 0)\n",
        "    true_cluster = train_df[train_df[\"categories\"] == true_category][\"cluster\"].iloc[0]\n",
        "\n",
        "    for k in k_values:\n",
        "        # CATEGORY-BASED EVALUATION\n",
        "        local_metrics[k][\"category\"][\"precision\"] += precision_at_k(recommendations, true_category, k)\n",
        "        local_metrics[k][\"category\"][\"recall\"] += recall_at_k(recommendations, true_category, all_relevant_count, k)\n",
        "        local_metrics[k][\"category\"][\"mrr\"] += mean_reciprocal_rank(recommendations, true_category, k)\n",
        "\n",
        "        # CLUSTERING-BASED EVALUATION\n",
        "        predicted_clusters = recommendations[\"categories\"].map(lambda cat: train_df[train_df[\"categories\"] == cat][\"cluster\"].iloc[0])\n",
        "        relevant_clusters = sum(1 for cluster in predicted_clusters[:k] if cluster == true_cluster)\n",
        "        local_metrics[k][\"clustering\"][\"precision\"] += relevant_clusters / k\n",
        "        local_metrics[k][\"clustering\"][\"recall\"] += relevant_clusters / all_relevant_count if all_relevant_count > 0 else 0\n",
        "        local_metrics[k][\"clustering\"][\"mrr\"] += mean_reciprocal_rank(recommendations, true_cluster, k)\n",
        "\n",
        "        # SIMILARITY-BASED EVALUATION\n",
        "        cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), recommendation_vectors)[0]\n",
        "        relevant_similar = sum(1 for score in cosine_similarities[:k] if score >= similarity_threshold)\n",
        "        local_metrics[k][\"similarity\"][\"precision\"] += relevant_similar / k\n",
        "        local_metrics[k][\"similarity\"][\"recall\"] += relevant_similar / all_relevant_count if all_relevant_count > 0 else 0\n",
        "        local_metrics[k][\"similarity\"][\"mrr\"] += mean_reciprocal_rank(recommendations, true_category, k)\n",
        "\n",
        "        # TEMPORAL-BASED EVALUATION\n",
        "        relevant_temporal = sum(1 for rec_date in recommendations[\"update_date\"][:k] if temporal_score(query_date, rec_date))\n",
        "        local_metrics[k][\"temporal\"][\"precision\"] += relevant_temporal / k\n",
        "        local_metrics[k][\"temporal\"][\"recall\"] += relevant_temporal / all_relevant_count if all_relevant_count > 0 else 0\n",
        "        local_metrics[k][\"temporal\"][\"mrr\"] += mean_reciprocal_rank(recommendations, true_category, k)\n",
        "\n",
        "    return local_metrics\n",
        "\n",
        "# Process queries in parallel\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    futures = [\n",
        "        executor.submit(process_query, idx, row[\"cleaned_abstract\"], row[\"categories\"], pd.to_datetime(row[\"update_date\"], errors='coerce'))\n",
        "        for idx, row in test_df.iterrows()\n",
        "    ]\n",
        "\n",
        "    for future in tqdm(as_completed(futures), total=num_queries, desc=\"Processing Results\"):\n",
        "        result = future.result()\n",
        "        with lock:\n",
        "            for k in k_values:\n",
        "                for method in result[k]:\n",
        "                    for metric in result[k][method]:\n",
        "                        metrics[k][method][metric] += result[k][method][metric]\n",
        "\n",
        "# Compute average metrics\n",
        "for k in k_values:\n",
        "    for method in metrics[k]:\n",
        "        for metric in metrics[k][method]:\n",
        "            metrics[k][method][metric] /= num_queries\n",
        "\n",
        "# Display final metrics\n",
        "print(\"\\nFinal Metrics:\")\n",
        "for k in k_values:\n",
        "    print(f\"\\nMetrics for k={k}:\")\n",
        "    for method, scores in metrics[k].items():\n",
        "        print(f\"\\n{method.capitalize()} Ground Truth:\")\n",
        "        print(f\" - Average Precision@{k}: {scores['precision']:.2f}\")\n",
        "        print(f\" - Average Recall@{k}: {scores['recall']:.2f}\")\n",
        "        print(f\" - Average MRR@{k}: {scores['mrr']:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3exhZzI48lKb",
        "outputId": "601d9e37-4735-42b6-c5c3-70a14b79d29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Results:  75%|ââââââââ  | 11221/14974 [2:03:15<22:15,  2.81it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sklearn.cluster import KMeans\n",
        "# from datetime import datetime\n",
        "# import lancedb\n",
        "\n",
        "# # Load the SBERT model\n",
        "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # Connect to LanceDB and open the table\n",
        "# db = lancedb.connect(\"lancedb_directory\")\n",
        "# table = db.open_table(\"enhanced_papers\")\n",
        "\n",
        "# # Load the training data from CSV (for metadata like categories)\n",
        "# train_df = pd.read_csv(\"train_df.csv\")\n",
        "\n",
        "# # Perform clustering on train embeddings for clustering-based ground truth\n",
        "# train_embeddings = np.vstack(table.to_pandas()[\"embedding\"].tolist())\n",
        "# num_clusters = 20  # Adjust the number of clusters as needed\n",
        "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "# train_df['cluster'] = kmeans.fit_predict(train_embeddings)\n",
        "\n",
        "# # Define temporal evaluation parameters\n",
        "# time_window_days = 365  # 1-year window for temporal evaluation\n",
        "\n",
        "# # Similarity threshold\n",
        "# similarity_threshold = 0.7\n",
        "\n",
        "# # Weights for hybrid scoring\n",
        "# weights = {\"category\": 0.3, \"cluster\": 0.2, \"similarity\": 0.3, \"temporal\": 0.2}\n",
        "\n",
        "# # Precompute category counts in the training data\n",
        "# category_counts = train_df[\"categories\"].value_counts().to_dict()\n",
        "\n",
        "# # Initialize metrics for each ground truth method and k values\n",
        "# k_values = [5, 10, 15]\n",
        "# metrics = {\n",
        "#     k: {\n",
        "#         \"category\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "#         \"clustering\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "#         \"similarity\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "#         \"temporal\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "#         \"hybrid\": {\"precision\": 0, \"recall\": 0, \"mrr\": 0},\n",
        "#     }\n",
        "#     for k in k_values\n",
        "# }\n",
        "\n",
        "# # Number of queries to process\n",
        "# num_queries = len(test_df)\n",
        "\n",
        "# # Define batch size\n",
        "# batch_size = 100\n",
        "\n",
        "# # Evaluation functions\n",
        "# def precision_at_k(recommendations, true_label, k):\n",
        "#     relevant = sum(1 for label in recommendations[\"categories\"].tolist()[:k] if label == true_label)\n",
        "#     return relevant / k\n",
        "\n",
        "# def recall_at_k(recommendations, true_label, all_relevant_count, k):\n",
        "#     relevant = sum(1 for label in recommendations[\"categories\"].tolist()[:k] if label == true_label)\n",
        "#     return relevant / all_relevant_count if all_relevant_count > 0 else 0\n",
        "\n",
        "# def mean_reciprocal_rank(recommendations, true_label, k):\n",
        "#     for i, label in enumerate(recommendations[\"categories\"].tolist()[:k]):\n",
        "#         if label == true_label:\n",
        "#             return 1 / (i + 1)\n",
        "#     return 0\n",
        "\n",
        "# def temporal_score(query_date, rec_date, window=time_window_days):\n",
        "#     rec_date = pd.to_datetime(rec_date, errors='coerce')\n",
        "#     return 1 if pd.notnull(rec_date) and abs((query_date - rec_date).days) <= window else 0\n",
        "\n",
        "# # Process queries in batches\n",
        "# for start in tqdm(range(0, num_queries, batch_size), desc=\"Processing Batches\"):\n",
        "#     end = min(start + batch_size, num_queries)\n",
        "\n",
        "#     # Extract batch queries, true categories, and update dates\n",
        "#     batch_queries = test_df.iloc[start:end][\"cleaned_abstract\"].tolist()\n",
        "#     batch_categories = test_df.iloc[start:end][\"categories\"].values\n",
        "#     batch_dates = pd.to_datetime(test_df.iloc[start:end][\"update_date\"], errors='coerce').values\n",
        "\n",
        "#     # Generate embeddings for the batch of queries\n",
        "#     query_embeddings = model.encode(batch_queries, batch_size=32, show_progress_bar=False)\n",
        "\n",
        "#     for idx, (query_embedding, true_category, query_date) in enumerate(zip(query_embeddings, batch_categories, batch_dates)):\n",
        "#         # Perform similarity search in LanceDB with a limit of 15\n",
        "#         recommendations = table.search(query_embedding).metric(\"cosine\").limit(15).to_pandas()\n",
        "#         recommendation_vectors = np.vstack(recommendations['embedding'].tolist())\n",
        "\n",
        "#         # Precompute true cluster and all relevant count\n",
        "#         all_relevant_count = category_counts.get(true_category, 0)\n",
        "#         true_cluster = train_df[train_df[\"categories\"] == true_category][\"cluster\"].iloc[0]\n",
        "\n",
        "#         for k in k_values:\n",
        "#             # CATEGORY-BASED EVALUATION\n",
        "#             metrics[k][\"category\"][\"precision\"] += precision_at_k(recommendations, true_category, k)\n",
        "#             metrics[k][\"category\"][\"recall\"] += recall_at_k(recommendations, true_category, all_relevant_count, k)\n",
        "#             metrics[k][\"category\"][\"mrr\"] += mean_reciprocal_rank(recommendations, true_category, k)\n",
        "\n",
        "#             # CLUSTERING-BASED EVALUATION\n",
        "#             predicted_clusters = recommendations[\"categories\"].map(lambda cat: train_df[train_df[\"categories\"] == cat][\"cluster\"].iloc[0])\n",
        "#             relevant_clusters = sum(1 for cluster in predicted_clusters[:k] if cluster == true_cluster)\n",
        "#             metrics[k][\"clustering\"][\"precision\"] += relevant_clusters / k\n",
        "#             metrics[k][\"clustering\"][\"recall\"] += relevant_clusters / all_relevant_count if all_relevant_count > 0 else 0\n",
        "#             metrics[k][\"clustering\"][\"mrr\"] += mean_reciprocal_rank(recommendations, true_cluster, k)\n",
        "\n",
        "#             # SIMILARITY-BASED EVALUATION\n",
        "#             cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), recommendation_vectors)[0]\n",
        "#             relevant_similar = sum(1 for score in cosine_similarities[:k] if score >= similarity_threshold)\n",
        "#             metrics[k][\"similarity\"][\"precision\"] += relevant_similar / k\n",
        "#             metrics[k][\"similarity\"][\"recall\"] += relevant_similar / all_relevant_count if all_relevant_count > 0 else 0\n",
        "#             metrics[k][\"similarity\"][\"mrr\"] += mean_reciprocal_rank(recommendations, true_category, k)\n",
        "\n",
        "#             # TEMPORAL-BASED EVALUATION\n",
        "#             relevant_temporal = sum(1 for rec_date in recommendations[\"update_date\"][:k] if temporal_score(query_date, rec_date))\n",
        "#             metrics[k][\"temporal\"][\"precision\"] += relevant_temporal / k\n",
        "#             metrics[k][\"temporal\"][\"recall\"] += relevant_temporal / all_relevant_count if all_relevant_count > 0 else 0\n",
        "#             metrics[k][\"temporal\"][\"mrr\"] += mean_reciprocal_rank(recommendations, true_category, k)\n",
        "\n",
        "#             # HYBRID SCORING\n",
        "#             hybrid_score = (\n",
        "#                 weights[\"category\"] * precision_at_k(recommendations, true_category, k) +\n",
        "#                 weights[\"cluster\"] * (relevant_clusters / k) +\n",
        "#                 weights[\"similarity\"] * (relevant_similar / k) +\n",
        "#                 weights[\"temporal\"] * (relevant_temporal / k)\n",
        "#             )\n",
        "#             metrics[k][\"hybrid\"][\"precision\"] += hybrid_score\n",
        "#             metrics[k][\"hybrid\"][\"recall\"] += hybrid_score\n",
        "#             metrics[k][\"hybrid\"][\"mrr\"] += hybrid_score\n",
        "\n",
        "# # Compute average metrics\n",
        "# for k in k_values:\n",
        "#     for method in metrics[k]:\n",
        "#         metrics[k][method][\"precision\"] /= num_queries\n",
        "#         metrics[k][method][\"recall\"] /= num_queries\n",
        "#         metrics[k][method][\"mrr\"] /= num_queries\n",
        "\n",
        "# # Display final metrics\n",
        "# print(\"\\nFinal Metrics:\")\n",
        "# for k in k_values:\n",
        "#     print(f\"\\nMetrics for k={k}:\")\n",
        "#     for method, scores in metrics[k].items():\n",
        "#         print(f\"\\n{method.capitalize()} Ground Truth:\")\n",
        "#         print(f\" - Average Precision@{k}: {scores['precision']:.2f}\")\n",
        "#         print(f\" - Average Recall@{k}: {scores['recall']:.2f}\")\n",
        "#         print(f\" - Average MRR@{k}: {scores['mrr']:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPU76Bi9zfiw",
        "outputId": "7b1779ad-4cb1-4761-ae78-370fa4de5587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batches: 100%|ââââââââââ| 50/50 [34:03<00:00, 40.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Metrics:\n",
            "\n",
            "Metrics for k=5:\n",
            "\n",
            "Category Ground Truth:\n",
            " - Average Precision@5: 0.58\n",
            " - Average Recall@5: 0.00\n",
            " - Average MRR@5: 0.71\n",
            "\n",
            "Clustering Ground Truth:\n",
            " - Average Precision@5: 0.70\n",
            " - Average Recall@5: 0.01\n",
            " - Average MRR@5: 0.00\n",
            "\n",
            "Similarity Ground Truth:\n",
            " - Average Precision@5: 0.18\n",
            " - Average Recall@5: 0.00\n",
            " - Average MRR@5: 0.71\n",
            "\n",
            "Temporal Ground Truth:\n",
            " - Average Precision@5: 0.19\n",
            " - Average Recall@5: 0.00\n",
            " - Average MRR@5: 0.71\n",
            "\n",
            "Hybrid Ground Truth:\n",
            " - Average Precision@5: 0.40\n",
            " - Average Recall@5: 0.40\n",
            " - Average MRR@5: 0.40\n",
            "\n",
            "Metrics for k=10:\n",
            "\n",
            "Category Ground Truth:\n",
            " - Average Precision@10: 0.55\n",
            " - Average Recall@10: 0.01\n",
            " - Average MRR@10: 0.72\n",
            "\n",
            "Clustering Ground Truth:\n",
            " - Average Precision@10: 0.68\n",
            " - Average Recall@10: 0.01\n",
            " - Average MRR@10: 0.00\n",
            "\n",
            "Similarity Ground Truth:\n",
            " - Average Precision@10: 0.14\n",
            " - Average Recall@10: 0.00\n",
            " - Average MRR@10: 0.72\n",
            "\n",
            "Temporal Ground Truth:\n",
            " - Average Precision@10: 0.18\n",
            " - Average Recall@10: 0.00\n",
            " - Average MRR@10: 0.72\n",
            "\n",
            "Hybrid Ground Truth:\n",
            " - Average Precision@10: 0.38\n",
            " - Average Recall@10: 0.38\n",
            " - Average MRR@10: 0.38\n",
            "\n",
            "Metrics for k=15:\n",
            "\n",
            "Category Ground Truth:\n",
            " - Average Precision@15: 0.54\n",
            " - Average Recall@15: 0.01\n",
            " - Average MRR@15: 0.72\n",
            "\n",
            "Clustering Ground Truth:\n",
            " - Average Precision@15: 0.67\n",
            " - Average Recall@15: 0.02\n",
            " - Average MRR@15: 0.00\n",
            "\n",
            "Similarity Ground Truth:\n",
            " - Average Precision@15: 0.11\n",
            " - Average Recall@15: 0.00\n",
            " - Average MRR@15: 0.72\n",
            "\n",
            "Temporal Ground Truth:\n",
            " - Average Precision@15: 0.17\n",
            " - Average Recall@15: 0.00\n",
            " - Average MRR@15: 0.72\n",
            "\n",
            "Hybrid Ground Truth:\n",
            " - Average Precision@15: 0.37\n",
            " - Average Recall@15: 0.37\n",
            " - Average MRR@15: 0.37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}